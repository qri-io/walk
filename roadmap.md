
# Project Objective
Define & maintain common, modular web crawling infrastructure that multiple projects can depend on and share the results of.

# Timelines
This project is being built on spare time across a number of groups & people who have a shared need for this type of infrastructure. Because it's being built on spare time, things will move in fits & starts. If you're interested in using this project or taking part it in, let us know! Even if you can't contribute, knowing about additional use cases can help us document our needs

# Milestones
We make use of (and try to maintain the accuracy of) [github milestones](https://github.com/qri-io/walk/milestones), which are the best way to track current progress of the project.

Proposed milestones (in *rough* order):

- [ ] Single application that can crawl a full web site or a simple list of URLs and store the results on disk
- [ ] Swappable workers (one of which uses Headless Chrome)
- [ ] Many pluggable result handlers (not just on-disk storage)
- [ ] Distributed operation with workers on separate machines
- [ ] Deploy it on a server somewhere to run regularly
- [ ] Long-running process that can be scheduled to do different domains, page lists, etc. at different schedules
- [ ] (HTTP?) API for setting up schedules/domains/page lists and querying results/stored data
- [ ] External push notifications (probably web hooks)
- [ ] Web UI for configuring schedules/domains/page lists

# Background

- [Use Cases](#use-cases)
- [Why not just use ____?](#why-not-just-use-____)

** **

# Use Cases
_how interested organizations are intending to use this software_

## [EDGI](https://envirodatagov.org)
EDGI is intending to spin this software up as a _running service_ for at least two groups that work with web crawler output: **Web Monitoring** and **Web Archiving** project.

### Web Monitoring Project
- Regular (in time) and consistent (in data and quality) captures of large list of URLs (30k and growing).
    - Proactive notification of captures that *differ* from the previous capture of the same URL (we don’t have this for any other data source, so it’s not a hard requirement)
    - The outputted data structure should make it easy to understand and traverse redirects (if we can query for historic data, then we should be able to query on that information). This impacts the above point about notifying differences.
    - “Regular” means >= 2/week (Versionista currently gets us every 2 days but occasionally fails; IA currently gets us every day.)
- Regular (in time) and complete (does a reasonably good job of getting all URLs) maps of all the pages in a site and the links between them.
    - “Pages” means things you might browse directly to. It includes, for example, PDFs, but not JS files.
    - “Regular” means >= 1/month.
    - General a word corpus here would be great, but we can also build that as higher level tool.
- Data generated by this service should be public (or handed to an entity that can make it public).
- It’s a live, running service (not just code), so the actual scraping is de-duplicated across all our projects that have scraping needs.

### Web Archiving Project
- Compliment the archives of existing sources like the Internet Archive by building infrastructure that can match common archiving formats & specs (eg: WARC files)
- Re-contextualize those same archives on the distributed web (eg: IPFS, Dat)
- Regular (in time) crawls of designated dataset & "grey material" URLS
- Have fewer crawlers running against these host services by using the same fetches as the web monitoring project.

## [DocNow](https://github.com/DocNow) & [Diffengine](https://github.com/DocNow/diffengine):

> In Slack @edsu and I had talked about the value of having a service like this for [docnow](https://github.com/DocNow) and [diffengine](https://github.com/DocNow/diffengine). If I understood his needs well:
- Regular (in time) captures of URLs mentioned in a curated set of RSS feeds
    - Not sure on specific definitions of “regular” for this use-case.
- Proactive notification of captures that *differ* from the previous capture of the same URL

- @mr0grog

# Why not just use ____?

There are lots of other tools and services for spidering, scraping, and monitoring web sites. Why build another one? The folks involved in this project have been relying on a variety of tools and services for this functionality so far, and this tool is our attempt to solve shortcomings we’ve encountered:

- **Frameworks like Scrapy and open-source apps like Klaxon:** The architecture is very similar, but we are aiming to make sure we can scale up to a distributed system that can do a *lot* of work here, which is not always well supported in other tools. We also want to add a lot of functionality on *top* of a generalized scraping framework, like:
    - storing results over a long term
    - generating a standard set of richer metadata
    - publishing push-style notifications to external systems, and
    - making special considerations when a snapshot differs from the previous snapshot of the same page.

    That said, it’s possible the end result of all this work could just be a lot of extensions and tooling built around something like Scrapy.

- **The Internet Archive:** We love the Internet Archive! BUT, it’s critical that we generate complete snapshots of entire websites and that we do so on a regular and frequent schedule. It’s hard to make sure that happens with the Internet Archive, because their scope is so broad.

- **Other private services, like Versionista:** We’ve used many of these services to support our own work for a long time! However, most folks working on this tool want to make sure our output is public, which usually involves a lot of extra work when using a third party service. At scale, many of these services can get expensive, too. Finally, we’re interested in having more control over and customization around this core piece of infrastructure.
